#include "network.h"
#include "detection_layer.h"
#include "region_layer.h"
#include "cost_layer.h"
#include "utils.h"
#include "parser.h"
#include "box.h"
#include "image.h"
#include "demo.h"
#include <time.h>

#include "libavcodec/avcodec.h"
#include "libavformat/avformat.h"
#include "libswscale/swscale.h"
#include "libavdevice/avdevice.h"

#include <opencv2/imgproc/imgproc_c.h>

#define DEMO 1

#ifdef OPENCV

static char **demo_names;
static image **demo_alphabet;
static int demo_classes;

static network *net;
static image buff[3];
static image buff_letter[3];
static int buff_index = 0;
static CvCapture * cap;
static IplImage  * ipl;
static float fps = 0;
static float demo_thresh = 0;
static float demo_hier = .5;
static int running = 0;

static int demo_frame = 3;
static int demo_index = 0;
static float **predictions;
static float *avg;
static int demo_done = 0;
static int demo_total = 0;
double demo_time;

detection *get_network_boxes(network *net, int w, int h, float thresh, float hier, int *map, int relative, int *num);

static void fill_iplimage_from_frame(IplImage *img, const AVFrame *frame, enum AVPixelFormat pixfmt, int width, int height)
{
	IplImage *tmpimg;
	int depth, channels_nb;
	tmpimg = cvCreateImageHeader((CvSize) { frame->width, frame->height }, 8, 3);
	*img = *tmpimg;
	img->imageData = img->imageDataOrigin = frame->data[0];
	img->dataOrder = IPL_DATA_ORDER_PIXEL;
	img->origin = IPL_ORIGIN_TL;
	img->widthStep = frame->linesize[0];
}

static void CopyDate(AVFrame *pFrame, int width, int height, int time);
static void SaveFrame(AVFrame *pFrame, int width, int height, int iFrame);


#define OUTPUT_YUV420P 0
#define SFM_REFRESH_EVENT  (SDL_USEREVENT + 1)
#define SFM_BREAK_EVENT  (SDL_USEREVENT + 2)
void show_dshow_device() {
	AVFormatContext *pFormatCtx = avformat_alloc_context();
	AVDictionary* options = NULL;
	av_dict_set(&options, "list_devices", "true", 0);
	AVInputFormat *iformat = av_find_input_format("dshow");
	printf("========Device Info=============\n");
	avformat_open_input(&pFormatCtx, "video=dummy", iformat, &options);
	printf("================================\n");
}

int size_network(network *net)
{
	int i;
	int count = 0;
	for (i = 0; i < net->n; ++i) {
		layer l = net->layers[i];
		if (l.type == YOLO || l.type == REGION || l.type == DETECTION) {
			count += l.outputs;
		}
	}
	return count;
}
int eggplant()
{

	av_register_all();
	avformat_network_init();

	//
	AVFormatContext *pFormatCtx = NULL;
	int             i, videoStream;
	AVCodecContext  *pCodecCtx;
	AVCodec         *pCodec;
	AVFrame         *pFrame;
	AVFrame         *pFrameRGB;
	AVPacket        packet;
	int             frameFinished;
	int             numBytes;
	uint8_t         *buffer;

	pFormatCtx = avformat_alloc_context();
	avdevice_register_all();

	// Register all formats and codecs  
	av_register_all();


	AVInputFormat *ifmt = av_find_input_format("dshow");
	if (avformat_open_input(&pFormatCtx, "video=screen-capture-recorder", ifmt, NULL) != 0) {
		printf("Couldn't open input stream.\n");
		//		return -1;
	}

	// Retrieve stream information  
	if (avformat_find_stream_info(pFormatCtx, NULL) < 0)
		//		return -1; // Couldn't find stream information  

		// Dump information about file onto standard error  
		//av_dump_format(pFormatCtx, 0, argv[1], false);
		av_dump_format(pFormatCtx, 0, "", 0);

	// Find the first video stream  
	videoStream = -1;
	for (i = 0; i < pFormatCtx->nb_streams; i++)
		if (pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO)
		{
			videoStream = i;
			break;
		}
	if (videoStream == -1)
		return -1; // Didn't find a video stream  

				   // Get a pointer to the codec context for the video stream  
	pCodecCtx = pFormatCtx->streams[videoStream]->codec;

	// Find the decoder for the video stream  
	pCodec = avcodec_find_decoder(pCodecCtx->codec_id);
	if (pCodec == NULL)
		return -1; // Codec not found  

				   // Open codec  
	if (avcodec_open2(pCodecCtx, pCodec, 0) < 0)
		return -1; // Could not open codec  

				   // Hack to correct wrong frame rates that seem to be generated by some codecs  
	if (pCodecCtx->time_base.num > 1000 && pCodecCtx->time_base.den == 1)
		pCodecCtx->time_base.den = 1000;

	// Allocate video frame  
	pFrame = av_frame_alloc();

	// Allocate an AVFrame structure  
	pFrameRGB = av_frame_alloc();
	if (pFrameRGB == NULL)
		return -1;

	// Determine required buffer size and allocate buffer  
	numBytes = avpicture_get_size(AV_PIX_FMT_RGB24, pCodecCtx->width,
		pCodecCtx->height);

	//buffer=malloc(numBytes);  
	buffer = (uint8_t *)av_malloc(numBytes * sizeof(uint8_t));

	// Assign appropriate parts of buffer to image planes in pFrameRGB  
	avpicture_fill((AVPicture *)pFrameRGB, buffer, AV_PIX_FMT_RGB24,
		pCodecCtx->width, pCodecCtx->height);

	//#########################################################################

	//#########################################################################
	// Read frames and save first five frames to disk  
	i = 0;
	long prepts = 0;
	while (av_read_frame(pFormatCtx, &packet) >= 0)
	{
		// Is this a packet from the video stream?  
		if (packet.stream_index == videoStream)
		{
			// Decode video frame  
			avcodec_decode_video2(pCodecCtx, pFrame, &frameFinished, &packet);

			// Did we get a video frame?  
			if (frameFinished)
			{
				static struct SwsContext *img_convert_ctx;

				// Convert the image into YUV format that SDL uses  
				if (img_convert_ctx == NULL) {
					int w = pCodecCtx->width;
					int h = pCodecCtx->height;

					img_convert_ctx = sws_getContext(w, h,
						pCodecCtx->pix_fmt,
						w, h, AV_PIX_FMT_RGB24, SWS_BICUBIC,
						NULL, NULL, NULL);
					if (img_convert_ctx == NULL) {
						fprintf(stderr, "Cannot initialize the conversion context!\n");
						exit(1);
					}
				}
				int ret = sws_scale(img_convert_ctx, pFrame->data, pFrame->linesize, 0,
					pCodecCtx->height, pFrameRGB->data, pFrameRGB->linesize);

				CopyDate(pFrameRGB, pCodecCtx->width, pCodecCtx->height, packet.pts - prepts);



				prepts = packet.pts;
			}
		}

		// Free the packet that was allocated by av_read_frame  
		av_free_packet(&packet);
	}

	// Free the RGB image  
	//free(buffer);  
	av_free(buffer);
	av_free(pFrameRGB);

	// Free the YUV frame  
	av_free(pFrame);

	// Close the codec  
	avcodec_close(pCodecCtx);

	// Close the video file  
	avformat_close_input(&pFormatCtx);

}

static void CopyDate(AVFrame *pFrame, int width, int height, int time)
{
	if (time <= 0) time = 1;

	IplImage *img;
	fill_iplimage_from_frame(&img, pFrame, AV_PIX_FMT_BGRA, width, height);

}


void remember_network(network *net)
{
	int i;
	int count = 0;
	for (i = 0; i < net->n; ++i) {
		layer l = net->layers[i];
		if (l.type == YOLO || l.type == REGION || l.type == DETECTION) {
			memcpy(predictions[demo_index] + count, net->layers[i].output, sizeof(float) * l.outputs);
			count += l.outputs;
		}
	}
}

detection *avg_predictions(network *net, int *nboxes)
{
	int i, j;
	int count = 0;
	fill_cpu(demo_total, 0, avg, 1);
	for (j = 0; j < demo_frame; ++j) {
		axpy_cpu(demo_total, 1. / demo_frame, predictions[j], 1, avg, 1);
	}
	for (i = 0; i < net->n; ++i) {
		layer l = net->layers[i];
		if (l.type == YOLO || l.type == REGION || l.type == DETECTION) {
			memcpy(l.output, avg + count, sizeof(float) * l.outputs);
			count += l.outputs;
		}
	}
	detection *dets = get_network_boxes(net, buff[0].w, buff[0].h, demo_thresh, demo_hier, 0, 1, nboxes);
	return dets;
}

void *detect_in_thread(void *ptr)
{
	running = 1;
	float nms = .4;

	layer l = net->layers[net->n - 1];
	float *X = buff_letter[(buff_index + 2) % 3].data;
	network_predict(net, X);

	/*
	   if(l.type == DETECTION){
	   get_detection_boxes(l, 1, 1, demo_thresh, probs, boxes, 0);
	   } else */
	remember_network(net);
	detection *dets = 0;
	int nboxes = 0;
	dets = avg_predictions(net, &nboxes);


	/*
	   int i,j;
	   box zero = {0};
	   int classes = l.classes;
	   for(i = 0; i < demo_detections; ++i){
	   avg[i].objectness = 0;
	   avg[i].bbox = zero;
	   memset(avg[i].prob, 0, classes*sizeof(float));
	   for(j = 0; j < demo_frame; ++j){
	   axpy_cpu(classes, 1./demo_frame, dets[j][i].prob, 1, avg[i].prob, 1);
	   avg[i].objectness += dets[j][i].objectness * 1./demo_frame;
	   avg[i].bbox.x += dets[j][i].bbox.x * 1./demo_frame;
	   avg[i].bbox.y += dets[j][i].bbox.y * 1./demo_frame;
	   avg[i].bbox.w += dets[j][i].bbox.w * 1./demo_frame;
	   avg[i].bbox.h += dets[j][i].bbox.h * 1./demo_frame;
	   }
	//copy_cpu(classes, dets[0][i].prob, 1, avg[i].prob, 1);
	//avg[i].objectness = dets[0][i].objectness;
	}
	 */

	if (nms > 0) do_nms_obj(dets, nboxes, l.classes, nms);

	printf("\033[2J");
	printf("\033[1;1H");
	printf("\nFPS:%.1f\n", fps);
	printf("Objects:\n\n");
	image display = buff[(buff_index + 2) % 3];
	draw_detections(display, dets, nboxes, demo_thresh, demo_names, demo_alphabet, demo_classes);
	free_detections(dets, nboxes);

	demo_index = (demo_index + 1) % demo_frame;
	running = 0;
	return 0;
}

void *fetch_in_thread(void *ptr)
{
	int status = fill_image_from_stream(cap, buff[buff_index]);
	letterbox_image_into(buff[buff_index], net->w, net->h, buff_letter[buff_index]);
	if (status == 0) demo_done = 1;
	return 0;
}

void *display_in_thread(void *ptr)
{
	show_image_cv(buff[(buff_index + 1) % 3], "Demo", ipl);
	int c = cvWaitKey(1);
	if (c != -1) c = c % 256;
	if (c == 27) {
		demo_done = 1;
		return 0;
	}
	else if (c == 82) {
		demo_thresh += .02;
	}
	else if (c == 84) {
		demo_thresh -= .02;
		if (demo_thresh <= .02) demo_thresh = .02;
	}
	else if (c == 83) {
		demo_hier += .02;
	}
	else if (c == 81) {
		demo_hier -= .02;
		if (demo_hier <= .0) demo_hier = .0;
	}
	return 0;
}

void *display_loop(void *ptr)
{
	while (1) {
		display_in_thread(0);
	}
}

void *detect_loop(void *ptr)
{
	while (1) {
		detect_in_thread(0);
	}
}

//
//
//AVCodec *pAvCodec = NULL;
//AVCodecContext *pAvCodecContext = NULL;
//AVFrame *pAvFrame = NULL;
//uint8_t endcode[] = { 0, 0, 1, 0xb7 };
//
//avcodec_register_all();
//
//pAvCodec = avcodec_find_encoder(AV_CODEC_ID_H264);
//
//pAvCodecContext = avcodec_alloc_context3(pAvCodec);
//
//pAvCodecContext->bit_rate = 200000;
///* resolution must be a multiple of two */
//pAvCodecContext->width = pFrame->width;
//pAvCodecContext->height = pFrame->height;
///* frames per second */
//pAvCodecContext->time_base.num = 1;
//pAvCodecContext->time_base.den = 15;
//pAvCodecContext->gop_size = 10; /* emit one intra frame every ten frames */
//pAvCodecContext->max_b_frames = 1;
//pAvCodecContext->thread_count = 20;
//pAvCodecContext->thread_type = FF_THREAD_FRAME;
//pAvCodecContext->pix_fmt = AV_PIX_FMT_YUV420P;
//av_opt_set(pAvCodecContext->priv_data, "preset", "slow", 0);
///* open it */
//if (avcodec_open2(pAvCodecContext, pAvCodec, NULL) < 0) {
//	fprintf(stderr, "Could not open codec\n");
//	return -1;
//}
//pAvFrame = avcodec_alloc_frame();
//if (!pAvFrame)
//{
//	fprintf(stderr, "Could not allocate video frame\n");
//	exit(1);
//}
//pAvFrame->format = pAvCodecContext->pix_fmt;
//pAvFrame->width = pAvCodecContext->width;
//pAvFrame->height = pAvCodecContext->height;
//
//ret = av_image_alloc(pAvFrame->data, pAvFrame->linesize, pAvCodecContext->width, pAvCodecContext->height,
//	pAvCodecContext->pix_fmt, pFrame->align);
//if (ret < 0) {
//	fprintf(stderr, "Could not allocate raw picture buffer\n");
//	return -1;
//}
//
//if (encode_frame(pAvCodecContext, pAvCodec, pAvFrame, pFrame, pFd, n, 1) < 0)
//{
//	return -1
//}

//
int switch_format(AVFrame *pYuvFrame, int nWidth, int nHeight, int nDataLen, char *pData, uint8_t *pYuvBuffer)
{
	/*
	AVFrame *pRgbFrame = NULL;
	pRgbFrame = new AVFrame[1];
	SwsContext * scxt = sws_getContext(nWidth, nHeight, AV_PIX_FMT_BGR24, nWidth, nHeight, AV_PIX_FMT_YUV420P, SWS_POINT, NULL, NULL, NULL);


	//AVFrame *m_pYUVFrame = new AVFrame[1];  
	avpicture_fill((AVPicture*)pRgbFrame, (uint8_t*)pData, AV_PIX_FMT_RGB24, nWidth, nHeight);


	//将YUV buffer 填充YUV Frame  
	avpicture_fill((AVPicture*)pYuvFrame, (uint8_t*)pYuvBuffer, AV_PIX_FMT_YUV420P, nWidth, nHeight);


	// 翻转RGB图像  
	// pRgbFrame->data[0]  += pRgbFrame->linesize[0] * (nHeight - 1);  
	// pRgbFrame->linesize[0] *= -1;                     
	// pRgbFrame->data[1]  += pRgbFrame->linesize[1] * (nHeight / 2 - 1);  
	// pRgbFrame->linesize[1] *= -1;  
	// pRgbFrame->data[2]  += pRgbFrame->linesize[2] * (nHeight / 2 - 1);  
	// pRgbFrame->linesize[2] *= -1;  
	//将RGB转化为YUV  
	if (sws_scale(scxt, pRgbFrame->data, pRgbFrame->linesize, 0, nHeight, pYuvFrame->data, pYuvFrame->linesize) < 0)
	{
		printf("Error\n");
	}
	if (pRgbFrame)
	{
		delete pRgbFrame;
	}
	*/
	return 0;
}

int encode_frame(AVCodecContext *pAVContext, AVCodec *pCodec, AVFrame *pAvFrame, IplImage *pImg, FILE *pFile, int nPts, int nCliCnt)
{
	/*
	int ret, got_output = 0;
	AVPacket nAvPkt;
	static int nSize = 0;
	uint8_t * pYuvBuff = NULL;
	int size = pImg->width * pImg->height;
	pYuvBuff = (uint8_t *)malloc((size * 3) / 2);
	switch_format(pAvFrame, pImg->width, pImg->height, pImg->imageSize, pImg->imageData, pYuvBuff);
	av_init_packet(&nAvPkt);
	nAvPkt.data = NULL;    // packet data will be allocated by the encoder
	nAvPkt.size = 0;
	pAvFrame->pts = nPts;
	ret = avcodec_encode_video2(pAVContext, &nAvPkt, pAvFrame, &got_output);
	if (ret < 0)
	{
		fprintf(stderr, "Error encoding frame\n");
		return -1;
	}
	if (got_output)
	{
		nSize += nAvPkt.size;
		// printf("Write Frame %d to file size %d total %d\n",nPts,nAvPkt.size,nSize);
		if (writetofile)
		{
			fwrite(nAvPkt.data, 1, nAvPkt.size, pFile);
		}
		else {
			for (int i = 0; i < nCliCnt; i++)
			{
				send_to_remote("10.0.0.5", 60000 + i, nAvPkt.data, nAvPkt.size, nPts);
			}
		}
		av_free_packet(&nAvPkt);
	}
	if (pYuvBuff)
	{
		free(pYuvBuff);
	}
	*/
	return 0;
}

//#######
void demo(char *cfgfile, char *weightfile, float thresh, int cam_index, const char *filename, char **names, int classes, int delay, char *prefix, int avg_frames, float hier, int w, int h, int frames, int fullscreen)
{
	//demo_frame = avg_frames;
	image **alphabet = load_alphabet();
	demo_names = names;
	demo_alphabet = alphabet;
	demo_classes = classes;
	demo_thresh = thresh;
	demo_hier = hier;
	printf("Demo\n");
	net = load_network(cfgfile, weightfile, 0);
	set_batch_network(net, 1);
	pthread_t detect_thread;
	pthread_t fetch_thread;

	srand(2222222);

	int i;
	demo_total = size_network(net);
	predictions = calloc(demo_frame, sizeof(float*));
	for (i = 0; i < demo_frame; ++i) {
		predictions[i] = calloc(demo_total, sizeof(float));
	}
	avg = calloc(demo_total, sizeof(float));

	if (filename) {
		printf("video file: %s\n", filename);
		cap = cvCaptureFromFile(filename);
	}
	else {
		cap = cvCaptureFromCAM(cam_index);

		if (w) {
			cvSetCaptureProperty(cap, CV_CAP_PROP_FRAME_WIDTH, w);
		}
		if (h) {
			cvSetCaptureProperty(cap, CV_CAP_PROP_FRAME_HEIGHT, h);
		}
		if (frames) {
			cvSetCaptureProperty(cap, CV_CAP_PROP_FPS, frames);
		}
	}

	if (!cap) error("Couldn't connect to webcam.\n");
	//eggplant();
	buff[0] = get_image_from_stream(cap);
	buff[1] = copy_image(buff[0]);
	buff[2] = copy_image(buff[0]);
	buff_letter[0] = letterbox_image(buff[0], net->w, net->h);
	buff_letter[1] = letterbox_image(buff[0], net->w, net->h);
	buff_letter[2] = letterbox_image(buff[0], net->w, net->h);
	ipl = cvCreateImage(cvSize(buff[0].w, buff[0].h), IPL_DEPTH_8U, buff[0].c);

	int count = 0;
	if (!prefix) {
		cvNamedWindow("Demo", CV_WINDOW_NORMAL);
		if (fullscreen) {
			cvSetWindowProperty("Demo", CV_WND_PROP_FULLSCREEN, CV_WINDOW_FULLSCREEN);
		}
		else {
			cvMoveWindow("Demo", 0, 0);
			// cvResizeWindow("Demo", 1352, 1013);
			cvResizeWindow("Demo", 400, 400);
		}
	}

	demo_time = what_time_is_it_now();

	while (!demo_done) {
		//	system("pause");
		buff_index = (buff_index + 1) % 3;
		if (pthread_create(&fetch_thread, 0, fetch_in_thread, 0)) error("Thread creation failed");
		if (pthread_create(&detect_thread, 0, detect_in_thread, 0)) error("Thread creation failed");
		if (!prefix) {
			fps = 1. / (what_time_is_it_now() - demo_time);
			demo_time = what_time_is_it_now();
			display_in_thread(0);
		}
		else {
			char name[256];
			sprintf(name, "%s_%08d", prefix, count);
			save_image(buff[(buff_index + 1) % 3], name);
		}
		pthread_join(fetch_thread, 0);
		pthread_join(detect_thread, 0);
		++count;
	}

}

/*
   void demo_compare(char *cfg1, char *weight1, char *cfg2, char *weight2, float thresh, int cam_index, const char *filename, char **names, int classes, int delay, char *prefix, int avg_frames, float hier, int w, int h, int frames, int fullscreen)
   {
   demo_frame = avg_frames;
   predictions = calloc(demo_frame, sizeof(float*));
   image **alphabet = load_alphabet();
   demo_names = names;
   demo_alphabet = alphabet;
   demo_classes = classes;
   demo_thresh = thresh;
   demo_hier = hier;
   printf("Demo\n");
   net = load_network(cfg1, weight1, 0);
   set_batch_network(net, 1);
   pthread_t detect_thread;
   pthread_t fetch_thread;

   srand(2222222);

   if(filename){
   printf("video file: %s\n", filename);
   cap = cvCaptureFromFile(filename);
   }else{
   cap = cvCaptureFromCAM(cam_index);

   if(w){
   cvSetCaptureProperty(cap, CV_CAP_PROP_FRAME_WIDTH, w);
   }
   if(h){
   cvSetCaptureProperty(cap, CV_CAP_PROP_FRAME_HEIGHT, h);
   }
   if(frames){
   cvSetCaptureProperty(cap, CV_CAP_PROP_FPS, frames);
   }
   }

   if(!cap) error("Couldn't connect to webcam.\n");

   layer l = net->layers[net->n-1];
   demo_detections = l.n*l.w*l.h;
   int j;

   avg = (float *) calloc(l.outputs, sizeof(float));
   for(j = 0; j < demo_frame; ++j) predictions[j] = (float *) calloc(l.outputs, sizeof(float));

   boxes = (box *)calloc(l.w*l.h*l.n, sizeof(box));
   probs = (float **)calloc(l.w*l.h*l.n, sizeof(float *));
   for(j = 0; j < l.w*l.h*l.n; ++j) probs[j] = (float *)calloc(l.classes+1, sizeof(float));

   buff[0] = get_image_from_stream(cap);
   buff[1] = copy_image(buff[0]);
   buff[2] = copy_image(buff[0]);
   buff_letter[0] = letterbox_image(buff[0], net->w, net->h);
   buff_letter[1] = letterbox_image(buff[0], net->w, net->h);
   buff_letter[2] = letterbox_image(buff[0], net->w, net->h);
   ipl = cvCreateImage(cvSize(buff[0].w,buff[0].h), IPL_DEPTH_8U, buff[0].c);

   int count = 0;
   if(!prefix){
   cvNamedWindow("Demo", CV_WINDOW_NORMAL);
   if(fullscreen){
   cvSetWindowProperty("Demo", CV_WND_PROP_FULLSCREEN, CV_WINDOW_FULLSCREEN);
   } else {
   cvMoveWindow("Demo", 0, 0);
   cvResizeWindow("Demo", 1352, 1013);
   }
   }

   demo_time = what_time_is_it_now();

   while(!demo_done){
buff_index = (buff_index + 1) %3;
if(pthread_create(&fetch_thread, 0, fetch_in_thread, 0)) error("Thread creation failed");
if(pthread_create(&detect_thread, 0, detect_in_thread, 0)) error("Thread creation failed");
if(!prefix){
	fps = 1./(what_time_is_it_now() - demo_time);
	demo_time = what_time_is_it_now();
	display_in_thread(0);
}else{
	char name[256];
	sprintf(name, "%s_%08d", prefix, count);
	save_image(buff[(buff_index + 1)%3], name);
}
pthread_join(fetch_thread, 0);
pthread_join(detect_thread, 0);
++count;
}
}
*/
#else
void demo(char *cfgfile, char *weightfile, float thresh, int cam_index, const char *filename, char **names, int classes, int delay, char *prefix, int avg, float hier, int w, int h, int frames, int fullscreen)
{
	fprintf(stderr, "Demo needs OpenCV for webcam images.\n");
}
#endif

